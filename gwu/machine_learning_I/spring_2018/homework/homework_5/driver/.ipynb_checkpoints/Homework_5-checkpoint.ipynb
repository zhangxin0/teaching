{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\"> \n",
    "DATS 6202, Fall 2018, Homework_5\n",
    "</h1> \n",
    "\n",
    "<h4 align=\"center\"> \n",
    "Yuxiao Huang ([yuxiaohuang@gwu.edu](mailto:yuxiaohuang@gwu.edu))\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Note\n",
    "- Complete the missing parts indicated by # Implement me\n",
    "- Submit an ipynb file named Homework_5.ipynb to [blackboard](https://blackboard.gwu.edu) folder /Assignments/Homework_5/\n",
    "- We expect you to follow a reasonable programming style. While we do not mandate a specific style, we require that your code to be neat, clear, **documented/commented** and above all consistent. **Marks will be deducted if these are not followed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement: Some of the text and code below are from Exercise_10_solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some conversion between you and your engineer friend...\n",
    "$\\textbf{Friend}$: Thanks again for solving the XOR problem only using perceptrons! You are really awesome!\n",
    "\n",
    "$\\textbf{You}$: I know.\n",
    "\n",
    "$\\textbf{Friend}$: ... Have you ever worked on Iris?\n",
    "\n",
    "$\\textbf{You}$: Who hasn't?\n",
    "\n",
    "$\\textbf{Friend}$: ... I heard that missing values can cause a lot of troubles.\n",
    "\n",
    "$\\textbf{You}$: Not necessarily. You can still do things with them.\n",
    "\n",
    "$\\textbf{Friend}$: Oh really? What if 90% of the class labels are missing? Can you predict them?\n",
    "\n",
    "$\\textbf{You}$: No.\n",
    "\n",
    "$\\textbf{Friend}$: Well, you know what? Don't feel bad about yourself. You don't have to know everything.\n",
    "\n",
    "$\\textbf{You}$: What I am saying is, I do not even need 10% labels. Remove all the labels if you want and leave only three unique ones. I can give you over 70% prediction accuracy.\n",
    "\n",
    "$\\textbf{Friend}$: Only three labels? Are you serious?\n",
    "\n",
    "$\\textbf{You}$: Positive. The only problem is, I am a data scientist. I do not create missing values on purpose. You do that. Then I will go from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Your friend diligently removed labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "                 header=None,\n",
    "                 names=['sepal length', 'sepal width', 'petal length', 'petal width', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Get the feature and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = df[['sepal length', 'sepal width', 'petal length', 'petal width']].values, df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Encode the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Divide the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train before removing the class labels:\n",
      "[2 2 2 0 0 0 1 2 1 2 0 1 1 1 0 0 2 1 1 2 2 1 0 0 1 1 0 1 2 2 2 1 2 2 0 0 0\n",
      " 1 0 0 2 1 2 0 0 0 1 1 0 1 1 1 2 0 1 1 1 1 2 0 1 2 1 1 2 1 2 0 1 2 2 2 2 0\n",
      " 2 0 0 2 1 0 0 0 0 0 1 2 2 2 0 2 0 0 1 1 1 1 0 2 2 0 2 1 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "# Print y_train\n",
    "print('y_train before removing the class labels:')\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Get the index of rows containing the three unique class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "classes, indices = np.unique(y_train, return_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Remove the classes (by chaining them to -1) in rows where the classes have already appeared\n",
    "In the end, y_train contains only three unique class labels. The removed classes are denoted by '-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train after removing the labels:\n",
      "[ 2 -1 -1  0 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array([y_train[i] if i in indices else -1 for i in range(y_train.shape[0])])\n",
    "\n",
    "# Print y_train\n",
    "print('y_train after removing the labels:')\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Your turn\n",
    "Hint: You are expected to use KMeans to handle this problem. The idea is as follows:\n",
    "1. When applying KMeans on the training data, we get the cluster index for each sample (in the training data), some of which have class labels (that are not -1). Thus we can obtain a map (a dictionary in essence) from cluster index to class.\n",
    "2. When applying KMeans on the testing data, we get the cluster index for each sample (in the testing data). Using the map above, we can obtain the predicted classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. The KMeans model\n",
    "Hint: Consider the number of class labels (in the data) when deciding the value of n_clusters for KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. On the training data, compute cluster centers and predict cluster indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = km.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Create the map from cluster index to class label (that is not -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_ = {}\n",
    "for i in range(y_train.shape[0]):\n",
    "    if y_train[i] != -1:\n",
    "        key = y_train_pred[i]\n",
    "        val = y_train[i]\n",
    "        dict_[key] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. On the testing data, predict cluster indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = km.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. On the testing data, transform the cluster indices into class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.array([dict_[y_test_pred[i]] for i in range(y_test_pred.shape[0])])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7555555555555555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print('Accuracy:', end=' ')\n",
    "print(precision_recall_fscore_support(y_test_pred, y_test, average='micro')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: You go beyond Exercise_10 (to improve the prediction accuracy)\n",
    "Hint:\n",
    "1. You may consider to do the same thing for y_train_pred (obtained in step 2.2), as what you did for y_test_pred (in step 2.5).\n",
    "2. In turn, every sample in y_train_pred has a predicted class label.\n",
    "3. As a result, you can first fit a classifier from X_train and y_train_pred, then use the model to predict the class label for X_test. The choice for the classifiers and their hyperparameter settings are up to you.\n",
    "4. You may consider to tweak the solution for Exercise_9 to handle this problem. However, as we discussed previously, please do not forget to cite the source of the code.\n",
    "5. <font color='red'>Students who get the highest accuracy will receive 1 bonus point (which will be added to their final grade).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement me"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
