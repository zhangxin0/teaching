{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\"> \n",
    "DATS 6103 - 10, Summer 2018, Homework_2_solution\n",
    "</h1> \n",
    "\n",
    "<h1 align=\"center\"> \n",
    "Due June 3, 11:59 PM\n",
    "</h1> \n",
    "\n",
    "<h4 align=\"center\"> \n",
    "Author: Yuxiao Huang ([yuxiaohuang@gwu.edu](mailto:yuxiaohuang@gwu.edu))\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Note\n",
    "- Complete the missing parts indicated by \"# Implement me\" in cells **In [4] and In [5]**\n",
    "- Submit an ipynb file named Homework_2.ipynb to [blackboard](https://blackboard.gwu.edu) folder /Assignments/Homework_2/\n",
    "-  We expect you to follow a reasonable programming style. While we do not mandate a specific style, we require that your code to be neat, clear, **documented/commented** and above all consistent. **Marks will be deducted if these are not followed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Homework 1, we discussed how to standardize the training and testing sets. Here, we define our own class StandardScaler (as oppose to the one in Sklearn) to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Iris dataset\n",
    "Again, let us use [Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal length</th>\n",
       "      <th>Sepal width</th>\n",
       "      <th>Petal length</th>\n",
       "      <th>Petal width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal length  Sepal width  Petal length  Petal width       target\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "                 header=None, \n",
    "                 names=['Sepal length', 'Sepal width', 'Petal length', 'Petal width', 'target'])\n",
    "\n",
    "# Show the first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the feature and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the name of the features\n",
    "features = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width']\n",
    "\n",
    "# Get the feature vector\n",
    "X = df[features]\n",
    "\n",
    "# Get the name of the target\n",
    "taget = 'target'\n",
    "\n",
    "# Get the target vector\n",
    "y = df[taget]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Homework_1, we defined our own function, train_test_split, to divide the data into training and testing sets. Here, we use the [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement our StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class StandardScaler():\n",
    "    def __init__(self):\n",
    "        self.means = None\n",
    "        self.stds = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the mean and standard deviation of each feature in the data,\n",
    "        and update self.means and self.stds.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
    "            The data used to compute the mean and standard deviation          \n",
    "        \"\"\"\n",
    "        \n",
    "        # Update the mean of each feature in the data\n",
    "        self.means = np.mean(X)\n",
    "\n",
    "        # Update the std of each feature in the data\n",
    "        self.stds = np.std(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Standardize each feature in the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data that will be standardized.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X_std : array-like, the standardization of X\n",
    "        \"\"\"      \n",
    "        \n",
    "        # Calculate the standardization of each feature in the data\n",
    "        X_std = (X - self.means) / self.stds\n",
    "        \n",
    "        return X_std\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the mean and standard deviation of each feature in the data,\n",
    "        and update self.means and self.stds,\n",
    "        and standardize each feature in the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
    "            The data used to compute the mean and standard deviation, and perfrom the standardization          \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_std : array-like, the standardization of X\n",
    "        \"\"\"\n",
    "        \n",
    "        # See fit()\n",
    "        self.fit(X)\n",
    "        \n",
    "        # See transform()\n",
    "        X_std = self.transform(X)\n",
    "        \n",
    "        return X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare the StandardScaler object\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# Standardize each feature in the training set\n",
    "X_train_std = std_scaler.fit_transform(X_train)\n",
    "\n",
    "# Standardize each feature in the testing set\n",
    "X_test_std = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) in [scikit-learn](http://scikit-learn.org/stable/index.html)\n",
    "Here we compare the performance of our StandardScaler with the StandardScaler in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Declare the StandardScaler object\n",
    "StdScaler = StandardScaler()\n",
    "\n",
    "# Standardize X_train using the .fit_transform() function\n",
    "X_train_std_sklearn = StdScaler.fit_transform(X_train)\n",
    "\n",
    "# Standardize X_test using the .transform() function\n",
    "X_test_std_sklearn = StdScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the training set: 1.2615452158e-30\n",
      "MSE on the testing set: 1.26347623053e-30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calcuate and display the Mean Square Error (MSE)\n",
    "# based on the standardization produced by StandardScaler and that produced by your implementation\n",
    "# Since the seed of the pseudo random number generator was specified,\n",
    "# you should see the exact result when your implementation is correct\n",
    "print('MSE on the training set: ' + str(mean_squared_error(X_train_std_sklearn, X_train_std)))\n",
    "print('MSE on the testing set: ' + str(mean_squared_error(X_test_std_sklearn, X_test_std)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
