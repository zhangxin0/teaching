{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\"> \n",
    "DATS 6202, Spring 2018, Homework_4_solution\n",
    "</h1> \n",
    "\n",
    "<h1 align=\"center\"> \n",
    "Due April 23, 11:59 PM\n",
    "</h1> \n",
    "\n",
    "<h4 align=\"center\"> \n",
    "Yuxiao Huang ([yuxiaohuang@gwu.edu](mailto:yuxiaohuang@gwu.edu))\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Note\n",
    "- Complete the missing parts indicated by **# Implement me**\n",
    "- Submit an ipynb file named **Homework_4.ipynb** to [blackboard](https://blackboard.gwu.edu) folder /Assignments/Homework_4/\n",
    "-  We expect you to follow a reasonable programming style. While we do not mandate a specific style, we require that your code to be neat, clear, **documented/commented** and above all consistent. **Marks will be deducted if these are not followed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some conversion between you and your engineer friend...\n",
    "$\\textbf{Friend}$: Thanks again for solving the XOR problem only using perceptrons! You are really awesome!\n",
    "\n",
    "$\\textbf{You}$: I know.\n",
    "\n",
    "$\\textbf{Friend}$: ... Have you ever worked on Iris?\n",
    "\n",
    "$\\textbf{You}$: Who hasn't?\n",
    "\n",
    "$\\textbf{Friend}$: ... Can you explain string theory?\n",
    "\n",
    "$\\textbf{You}$: Sure. Basically it is a unified theory that aims to explain every natural phenomenon. It claims that there are more than three dimensions in reality, most of which are too small to be observed. Actually, the math works pretty well when there are 10 dimensions. Now let us look at the equations...\n",
    "\n",
    "$\\textbf{Friend}$: Stop! Let us go back to Iris.\n",
    "\n",
    "$\\textbf{You}$: Sure.\n",
    "\n",
    "$\\textbf{Friend}$: ... I heard that missing values can cause a lot of troubles.\n",
    "\n",
    "$\\textbf{You}$: Not necessarily. You can still do things with them.\n",
    "\n",
    "$\\textbf{Friend}$: Oh really? What if 90% of the class labels are missing? Can you predict them?\n",
    "\n",
    "$\\textbf{You}$: No.\n",
    "\n",
    "$\\textbf{Friend}$: Well, you know what? Don't feel bad about yourself. It is ok. You don't have to know everything.\n",
    "\n",
    "$\\textbf{You}$: What I am saying is, I do not even need 10% labels. Remove all the labels if you want and leave only three unique ones. I can give you over 70% predict accuracy.\n",
    "\n",
    "$\\textbf{Friend}$: Only three labels? Are you serious?\n",
    "\n",
    "$\\textbf{You}$: Positive. The only problem is, I am a data scientist. I do not create missing values on purpose. You do that. Then I will go from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Your friend diligently removed labels\n",
    "1. In the end, y_train contains only three unique meaningful labels, the removed labels are denoted by '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def your_friends_diligent_work():\n",
    "    \"\"\"\n",
    "    :return: X_train, y_train, X_test, y_test\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read Iris\n",
    "    df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "                     header=None,\n",
    "                     names=['sepal length', 'sepal width', 'petal length', 'petal width', 'target'])\n",
    "\n",
    "    # Get the features and target\n",
    "    X, y = df[['sepal length', 'sepal width', 'petal length', 'petal width']], df['target']\n",
    "\n",
    "    # Encode the target\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    # Divide the data into training and testing data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Get the index of rows containing the three unique labels\n",
    "    yu, idxs = np.unique(y_train, return_index=True)\n",
    "    \n",
    "    # Remove labels\n",
    "    for i in range(len(y_train)):\n",
    "        if i not in idxs:\n",
    "            y_train[i] = -1\n",
    "                \n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Your first effort\n",
    "1. You did not use any other packages\n",
    "2. The prediction accuracy on y_test is over 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.75555555555555554, 0.75555555555555554, 0.75555555555555554, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Get the training and testing data\n",
    "# y_train contains only three unique meaningful labels, the removed labels are denoted by '-1'\n",
    "X_train, X_test, y_train, y_test = your_friends_diligent_work()\n",
    "\n",
    "# The KMeans classifier\n",
    "km = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "# Compute cluster centers and predict cluster indexs\n",
    "y_train_pred = km.fit_predict(X_train)\n",
    "\n",
    "# Create the map where the key is the predicted cluster index, value the real label\n",
    "map = {}\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] != -1:\n",
    "        key = y_train_pred[i]\n",
    "        val = y_train[i]\n",
    "        map[key] = val\n",
    "        \n",
    "# Transform the predicted cluster indexs into the real labels\n",
    "for i in range(len(y_train_pred)):\n",
    "    y_train_pred[i] = map[y_train_pred[i]]\n",
    "\n",
    "# Predict cluster indexs    \n",
    "y_test_pred = km.predict(X_test)\n",
    "\n",
    "# Transform the predicted cluster indexs into the real labels\n",
    "for i in range(len(y_test_pred)):\n",
    "    y_test_pred[i] = map[y_test_pred[i]]\n",
    "    \n",
    "print(precision_recall_fscore_support(y_test_pred, y_test, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: You went an extra mile\n",
    "1. You did not use any other packages\n",
    "2. You did use some results in Step 2\n",
    "3. You tried something mentioned in Amir's talk in ML I\n",
    "4. The prediction accuracy on y_test is improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.77777777777777779, 0.77777777777777779, 0.77777777777777779, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yhuang/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# The MLP classifier\n",
    "mlp = MLPClassifier(random_state=0)\n",
    "\n",
    "# Train\n",
    "mlp.fit(X_train, y_train_pred)\n",
    "\n",
    "# Test\n",
    "y_test_pred = mlp.predict(X_test)\n",
    "\n",
    "print(precision_recall_fscore_support(y_test_pred, y_test, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "1. Part of the code is from the \"Python Machine Learning (2nd edition)\" book code repository\n",
    "2. Please find the reference to and website of the book below:\n",
    "    - Raschka S. and Mirjalili V. (2017). Python Machine Learning. 2nd Edition.\n",
    "    - https://sebastianraschka.com/books.html\n",
    "3. Please find the website of the book code repository and info resource below:\n",
    "    - https://github.com/rasbt/python-machine-learning-book-2nd-edition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
